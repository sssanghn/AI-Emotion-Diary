{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHM57OhBk/YYJ/abPVKBRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssanghn/AI-Emotion-Diary/blob/main/KoBERT_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBERT 감정 다중분류모델"
      ],
      "metadata": {
        "id": "XE0cy30W52Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> KoBERT를 이용하여 한국어 문장을 여러 클래스로 분류하는 모델을 만들어 보려고 한다.\n",
        "</br>\n",
        "공포, 놀람, 분노, 슬픔, 중립, 기쁨, 혐오와 같은 감정이 느껴지는 짧은 대화 텍스트를 각각 어떠한 감정의 텍스트인지 분류하는 모델을 만드는 것이다. </br>\n",
        "AIHUB에서 가져온 데이터셋으로 KoBERT 모델을 FineTuning 하여 </br>\n",
        "최종적으로 AI 감정일기 어플리케이션에서 작성한 일기 텍스트에서 감정을 추출하기 위해 프로젝트를 진행하였다."
      ],
      "metadata": {
        "id": "pyb4ZGUx5uRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoBERT란?**\n",
        "\n",
        "> BERT는 약 33억 개의 단어로 pretrain 되어 있는 기계번역 모델이다. </br>\n",
        "하지만 외국에서 만든 것이다 보니 영어에 대해서는 정확도는 높고, 한국어에 대해서는 정확도가 떨어진다.</br>\n",
        "따라서 BERT 모델을 한국어에도 잘 활용할 수 있도록 만들어진 것이 KoBERT이다. </br>\n",
        "KoBERT는 BERT 모델에서 한국어 데이터를 추가로 학습시킨 모델로, 한국어 위키에서 5백만개의 문장과 5천4백만개의 단어를 학습시킨 모델이다. </br>\n",
        "따라서 한국어 데이터에 대해서도 높은 정확도를 낼 수 있다고 한다. </br>\n",
        "한국어의 불규칙한 언어 변화의 특성을 반영하기 위해 데이터 기반 토큰화 (Tokenization) 기법을 적용하여 기존 대비 27%의 토큰만으로 2.6% 이상의 성능 향상을 이끌어 낸 모델이다. </br>"
      ],
      "metadata": {
        "id": "12P4-SnE6m5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 프로젝트 설명"
      ],
      "metadata": {
        "id": "wZ6mfcOv7_7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoBERT 파인튜닝**\n",
        "> BERT 모델은 자신의 사용 목적에 따라 파인튜닝(FineTuning)이 가능하다. </br>\n",
        "따라서 Output layer만 추가로 달아주면 원하는 결과를 출력해내는 기계번역 모델을 만들어 낼 수 있다. </br>\n",
        "이 점을 활용하여 KoBERT 모델을 FineTuning하여 최종적으로 일기 텍스트에서 감정을 추출해내는 모델을 새로 만들어 보려고 한다."
      ],
      "metadata": {
        "id": "kZHpaww98FmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 수집"
      ],
      "metadata": {
        "id": "ChrLMdP78T3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**데이터 수집**\n",
        "> 이번 프로젝트를 위해 사용한 한국어 대화 문장 데이터셋은 총 4개이다. </br>\n",
        "</br>\n",
        "1. 한국어 감정 정보가 포함된 단발성 대화 데이터셋\n",
        "* SNS 글 및 온라인 댓글에 대한 웹 크롤링을 실시하여 선정된 AIHUB의 공공 데이터셋\n",
        "* 약 3만 9천건의 데이터셋(기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n",
        "2. 한국어 감정 정보가 포함된 연속적 대화 데이터셋\n",
        "* SNS 글 및 온라인 댓글에 대한 웹 크롤링을 실시하여 선정된 AIHUB의 공공 데이터셋\n",
        "* 연속적 10,000개 대화세트, 단발성으로는 55,627 문장 데이터셋(기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n",
        "3. 감정 분류를 위한 대화 음성 데이터셋\n",
        "* AI HUB내 KAIST 인공지능연구소에서 구축한 감정 분류를 위한 대화 음성 데이터셋에서 '5차년도 2차 CSV'를 활용\n",
        "* 감성대화 어플리케이션을 이용하여 수집한 AIHUB의 공공 데이터셋\n",
        "* 일정 기간동안 사용자들이 어플리케이션과 자연스럽게 대화하고, 수집된 데이터를 정제 작업을 거쳐 선별\n",
        "* 7가지 감정(기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n"
      ],
      "metadata": {
        "id": "TIL1jkIi8VkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab 환경 설정"
      ],
      "metadata": {
        "id": "7lWJDQw9MH_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델을 구현하기에 앞서 코랩 환경설정을 진행하려고 한다. </br>\n",
        "해당 사양은 KoBERT 오픈소스 내 requirements.txt를 참고하였다."
      ],
      "metadata": {
        "id": "Vk3XBoPUMyEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 환경 설정\n",
        "# requirements : https://github.com/SKTBrain/KoBERT/blob/master/kobert_hf/requirements.txt\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install mxnet\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install transformers==4.8.2\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k77lj1w6MNwQ",
        "outputId": "0995223d-4095-4354-9380-17df44d2e480"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.23.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.11.17)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Using cached sentencepiece-0.1.91.tar.gz (500 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting transformers==4.8.2\n",
            "  Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (3.13.1)\n",
            "Collecting huggingface-hub==0.0.12 (from transformers==4.8.2)\n",
            "  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (23.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.8.2)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.8.2)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.8.2) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.8.2) (2023.11.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.8.2) (1.3.2)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 다음으로 깃허브 내 파일을 Colab으로 가져온다."
      ],
      "metadata": {
        "id": "a1cNqF8fNL9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/SKTBrain/KoBERT의 파일들을 Colab으로 다운로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "255FRpYqNOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그 다음으로는 해당 라이브러리들을 import 해준다."
      ],
      "metadata": {
        "id": "h26pFd-GPTsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "# Transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "# Setting Library\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "W0nK9ZNXPWhi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}