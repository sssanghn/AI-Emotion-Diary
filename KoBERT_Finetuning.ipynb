{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssanghn/AI-Emotion-Diary/blob/main/KoBERT_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE0cy30W52Ta"
      },
      "source": [
        "# KoBERT 감정 다중분류모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyb4ZGUx5uRS"
      },
      "source": [
        "> KoBERT를 이용하여 한국어 문장을 여러 클래스로 분류하는 모델을 만들어 보려고 한다.\n",
        "</br>\n",
        "공포, 놀람, 분노, 슬픔, 중립, 기쁨, 혐오와 같은 감정이 느껴지는 짧은 대화 텍스트를 각각 어떠한 감정의 텍스트인지 분류하는 모델을 만드는 것이다. </br>\n",
        "AIHUB에서 가져온 데이터셋으로 KoBERT 모델을 FineTuning하여 </br>\n",
        "AI 감정일기 어플리케이션에서 작성한 일기 텍스트에서 감정을 추출하기 위한 목적으로 프로젝트를 진행하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12P4-SnE6m5Y"
      },
      "source": [
        "**KoBERT란?**\n",
        "\n",
        "> BERT는 약 33억 개의 단어로 pretrain 되어 있는 기계번역 모델이다. </br>\n",
        "하지만 외국에서 만든 것이다 보니 영어에 대해서는 정확도는 높고, 한국어에 대해서는 정확도가 떨어진다.</br>\n",
        "따라서 BERT 모델을 한국어에도 잘 활용할 수 있도록 만들어진 것이 KoBERT이다. </br>\n",
        "KoBERT는 BERT 모델에서 한국어 데이터를 추가로 학습시킨 모델로, 한국어 위키에서 5백만개의 문장과 5천4백만개의 단어를 학습시킨 모델이다. </br>\n",
        "따라서 한국어 데이터에 대해서도 높은 정확도를 낼 수 있다고 한다. </br>\n",
        "한국어의 불규칙한 언어 변화의 특성을 반영하기 위해 데이터 기반 토큰화 (Tokenization) 기법을 적용하여 기존 대비 27%의 토큰만으로 2.6% 이상의 성능 향상을 이끌어 낸 모델이다. </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6mfcOv7_7y"
      },
      "source": [
        "# 1. 프로젝트 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZHpaww98FmI"
      },
      "source": [
        "**KoBERT 파인튜닝**\n",
        "> BERT 모델은 자신의 사용 목적에 따라 파인튜닝(FineTuning)이 가능하다. </br>\n",
        "따라서 Output layer만 추가로 달아주면 원하는 결과를 출력해내는 기계번역 모델을 만들어 낼 수 있다. </br>\n",
        "이 점을 활용하여 KoBERT 모델을 FineTuning하여 최종적으로 일기 텍스트에서 감정을 추출해내는 모델을 새로 만들어 보려고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChrLMdP78T3b"
      },
      "source": [
        "# 2. 데이터 수집"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIL1jkIi8VkT"
      },
      "source": [
        "**데이터 수집**\n",
        "> 이번 프로젝트를 위해 사용한 한국어 대화 문장 데이터셋은 총 3개이다. </br>\n",
        "</br>\n",
        "1. 한국어 감정 정보가 포함된 단발성 대화 데이터셋\n",
        "* SNS 글 및 온라인 댓글에 대한 웹 크롤링을 실시하여 선정된 AIHUB의 공공 데이터셋\n",
        "* 약 3만 9천건의 데이터 셋 (기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n",
        "2. 한국어 감정 정보가 포함된 연속적 대화 데이터셋\n",
        "* SNS 글 및 온라인 댓글에 대한 웹 크롤링을 실시하여 선정된 AIHUB의 공공 데이터셋\n",
        "* 약 1만 건(단발성 55,627문장)의 데이터 셋 (기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n",
        "3. 감성 대화 말뭉치\n",
        "* 한국어 감성 데이터셋을 상세한 감정 라벨링(감정 대분류와 소분류로 구분)과 함께 제공하는 AIHub의 공공 데이터셋\n",
        "* 약 4만 건의 데이터 셋 (기쁨, 슬픔, 당황, 분노, 불안, 상처의 6개 감정)\n",
        "* 데이터 병합을 위해 감정 대분류와 대화 문장을 남기고, 감정 소분류는 누락"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 데이터 전처리"
      ],
      "metadata": {
        "id": "vNgJ-f2Ccg7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**데이터 전처리**\n",
        ">"
      ],
      "metadata": {
        "id": "2wEWV9Q6cjsy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNJLehSh7QZb"
      },
      "source": [
        "> 한국어 단발성 대화 데이터셋 (7) </br>\n",
        ": 공포 놀람 분노 슬픔 중립 행복 혐오 </br>\n",
        "한국어 연속적 대화 데이터셋 (7) </br>\n",
        ": 공포 놀람 분노 슬픔 중립 행복 혐오 </br>\n",
        "감성 대화 말뭉치 (6) </br>\n",
        ": 기쁨 당황 분노 불안 상처 슬픔 </br>\n",
        "</br>\n",
        "<병합된 결과> </br>\n",
        "한국어 단발성 대화 데이터셋 + 한국어 연속적 대화 데이터셋 + 감성 대화 말뭉치 (7) </br>\n",
        ": 공포 놀람 분노 행복 슬픔   중립 혐오 (7) </br>\n",
        ": 불안 (----)  분노 기쁨 슬픔,상처 (5) </br>\n",
        "\n",
        "\n",
        "데이터셋을 살펴본 결과, 감성 대화 말뭉치의 '당황' 감정은 한국어 단발성,연속적 대화 데이터셋의 '공포', '놀람' 감정과 중복된다. </br>\n",
        "따라서 두 데이터셋을 병합하고, 겹치지 않는 '당황' 감정은 제거하였다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data1 = pd.read_excel('/content/drive/MyDrive/dataset/한국어_단발성_대화_데이터셋.xlsx')\n",
        "\n",
        "data1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ByoTWUXhg0i7",
        "outputId": "d3a2be3d-696d-4f50-e264-21e70f85d324"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Sentence Emotion  Unnamed: 2  Unnamed: 3  Unnamed: 4  공포  \\\n",
              "0  언니 동생으로 부르는게 맞는 일인가요..??      공포         NaN         NaN         NaN  놀람   \n",
              "1              그냥 내 느낌일뿐겠지?      공포         NaN         NaN         NaN  분노   \n",
              "2            아직너무초기라서 그런거죠?      공포         NaN         NaN         NaN  슬픔   \n",
              "3             유치원버스 사고 낫다던데      공포         NaN         NaN         NaN  중립   \n",
              "4               근데 원래이런거맞나요      공포         NaN         NaN         NaN  행복   \n",
              "\n",
              "     5468  \n",
              "0  5898.0  \n",
              "1  5665.0  \n",
              "2  5267.0  \n",
              "3  4830.0  \n",
              "4  6037.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ded5038e-fd86-4f78-8d12-ae1612864e51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>공포</th>\n",
              "      <th>5468</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
              "      <td>공포</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>놀람</td>\n",
              "      <td>5898.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그냥 내 느낌일뿐겠지?</td>\n",
              "      <td>공포</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>분노</td>\n",
              "      <td>5665.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>아직너무초기라서 그런거죠?</td>\n",
              "      <td>공포</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>슬픔</td>\n",
              "      <td>5267.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>유치원버스 사고 낫다던데</td>\n",
              "      <td>공포</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>중립</td>\n",
              "      <td>4830.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>근데 원래이런거맞나요</td>\n",
              "      <td>공포</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>행복</td>\n",
              "      <td>6037.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ded5038e-fd86-4f78-8d12-ae1612864e51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ded5038e-fd86-4f78-8d12-ae1612864e51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ded5038e-fd86-4f78-8d12-ae1612864e51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ab235d8f-0c8f-4421-be02-56c215e79933\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab235d8f-0c8f-4421-be02-56c215e79933')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ab235d8f-0c8f-4421-be02-56c215e79933 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_excel('/content/drive/MyDrive/dataset/한국어_연속적_대화_데이터셋.xlsx')\n",
        "\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "2sSdU239gibG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3_train = pd.read_excel('/content/drive/MyDrive/dataset/감성대화말뭉치Training.xlsx')\n",
        "\n",
        "data3_train.head()"
      ],
      "metadata": {
        "id": "9AeYRqfrg5gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJwTrv_1nhJk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data1 = pd.read_excel('/content/drive/MyDrive/dataset/한국어_단발성_대화_데이터셋.xlsx')\n",
        "data2 = pd.read_excel('/content/drive/MyDrive/dataset/한국어_연속적_대화_데이터셋.xlsx')\n",
        "data3_train = pd.read_excel('/content/drive/MyDrive/dataset/감성대화말뭉치Training.xlsx')\n",
        "data3_validation = pd.read_excel('/content/drive/MyDrive/dataset/감성대화말뭉치Validation.xlsx')\n",
        "\n",
        "data3 = pd.concat([data3_train, data3_validation])\n",
        "\n",
        "\n",
        "# 감성 레이블을 정수로 변환 (0~6)\n",
        "data1.loc[(data1['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
        "data1.loc[(data1['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "data1.loc[(data1['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "data1.loc[(data1['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "data1.loc[(data1['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
        "data1.loc[(data1['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
        "data1.loc[(data1['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
        "\n",
        "data1_list = []\n",
        "for q, label in zip(data1['Sentence'], data1['Emotion'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data1_list.append(data)\n",
        "\n",
        "# 감성 레이블을 정수로 변환 (0~6)\n",
        "data2.loc[(data2['Unnamed: 2'] == \"공포\"), 'Unnamed: 2'] = 0  #공포 => 0\n",
        "data2.loc[(data2['Unnamed: 2'] == \"놀람\"), 'Unnamed: 2'] = 1  #놀람 => 1\n",
        "data2.loc[(data2['Unnamed: 2'] == \"분노\"), 'Unnamed: 2'] = 2  #분노 => 2\n",
        "data2.loc[(data2['Unnamed: 2'] == \"슬픔\"), 'Unnamed: 2'] = 3  #슬픔 => 3\n",
        "data2.loc[(data2['Unnamed: 2'] == \"중립\"), 'Unnamed: 2'] = 4  #중립 => 4\n",
        "data2.loc[(data2['Unnamed: 2'] == \"행복\"), 'Unnamed: 2'] = 5  #행복 => 5\n",
        "data2.loc[(data2['Unnamed: 2'] == \"혐오\"), 'Unnamed: 2'] = 6  #혐오 => 6\n",
        "\n",
        "# 데이터셋에서 'Unnamed' 열에 결측치가 있으므로 제거\n",
        "data2.dropna(subset = ['Unnamed: 2'])\n",
        "data2.drop(0, axis=0, inplace=True)\n",
        "\n",
        "data2_list = []\n",
        "for q, label in zip(data2['Unnamed: 1'], data2['Unnamed: 2']) :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data2_list.append(data)\n",
        "\n",
        "# 감성 레이블을 정수로 변환 (0, 2, 3, 5)\n",
        "data3.loc[(data3['감정_대분류'] == \"불안\"), '감정_대분류'] = 0 #불안 => 0\n",
        "data3.loc[(data3['감정_대분류'] == \"분노\"), '감정_대분류'] = 2 #분노 => 2\n",
        "data3.loc[(data3['감정_대분류'] == \"슬픔\"), '감정_대분류'] = 3 #슬픔 => 3\n",
        "data3.loc[(data3['감정_대분류'] == \"상처\"), '감정_대분류'] = 3 #상처 => 3\n",
        "data3.loc[(data3['감정_대분류'] == \"기쁨\"), '감정_대분류'] = 5 #기쁨 => 5\n",
        "\n",
        "# 데이터셋에서 나누어져있는 '사람문장1, 사람문장2, 사람문장3'을 병합, 당황 감정 제거\n",
        "data3 = data3.drop(data3[data3['감정_대분류'] == '당황'].index)\n",
        "data3['사람문장'] = data3['사람문장1'].astype(str) + data3['사람문장2'].astype(str) + data3['사람문장3'].astype(str)\n",
        "\n",
        "data3_list = []\n",
        "for q, label in zip(data3['사람문장'], data3['감정_대분류'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data3_list.append(data)\n",
        "\n",
        "merge_data = data1_list + data3_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lWJDQw9MH_K"
      },
      "source": [
        "# Colab 환경 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk3XBoPUMyEl"
      },
      "source": [
        "> 모델을 구현하기에 앞서 코랩 환경설정을 진행하려고 한다. </br>\n",
        "해당 사양은 KoBERT 오픈소스 내 requirements.txt를 참고하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k77lj1w6MNwQ"
      },
      "outputs": [],
      "source": [
        "# Colab 환경 설정\n",
        "# requirements : https://github.com/SKTBrain/KoBERT/blob/master/kobert_hf/requirements.txt\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1cNqF8fNL9l"
      },
      "source": [
        "> 다음으로 깃허브 내 파일을 Colab으로 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "255FRpYqNOSc"
      },
      "outputs": [],
      "source": [
        "# https://github.com/SKTBrain/KoBERT의 파일들을 Colab으로 다운로드\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26pFd-GPTsn"
      },
      "source": [
        "> 그 다음으로는 해당 라이브러리들을 import 해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0nK9ZNXPWhi"
      },
      "outputs": [],
      "source": [
        "# KoBERT\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "# Transformers\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "# Setting Library\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpfSUpg3ZQBQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgZxLJSwZgat"
      },
      "outputs": [],
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "\n",
        "# Setting parameters\n",
        "max_len = 128\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH7LJsWRNXMK"
      },
      "outputs": [],
      "source": [
        "class BERTSentenceTransform:\n",
        "    r\"\"\"BERT style data transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_seq_length, vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "        sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 2 strings:\n",
        "        text_a, text_b.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "\n",
        "        For single sequences, the input is a tuple of single string:\n",
        "        text_a.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
        "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
        "            string: (text_a,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "\n",
        "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
        "        # pre-training and are added to the wordpiece embedding vector\n",
        "        # (and position vector). This is not *strictly* necessary since\n",
        "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        #vocab = self._tokenizer.vocab\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbdF-_AzkQEJ"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYYxbIrokhtP"
      },
      "source": [
        "> BERT 데이터셋을 구성하는 라벨의 개수를 num_classes로 설정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXgT2KcX1vm"
      },
      "outputs": [],
      "source": [
        "# class BERTClassifier\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX70R4JFkWvd"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrgYhKJzkcmg"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "    dataset = [[sentence, '0']]\n",
        "    test = BERTDataset(dataset, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)\n",
        "    model.eval()\n",
        "    answer = 0\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        for logits in out:\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            answer = np.argmax(logits)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJN9dSjgB9VD"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = train_test_split(merge_data, test_size=0.2, shuffle=True, random_state=20)\n",
        "\n",
        "data_train = BERTDataset(train_set, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(test_set, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWYGE5HNnhlO"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq-zP0Gznqqq"
      },
      "outputs": [],
      "source": [
        "torch.save(model, f'/content/drive/MyDrive/Colab Notebooks/SentimentAnalysisKOBert.pt')\n",
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/SentimentAnalysisKOBert_StateDict.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QzzLStU34g1LMHFs-H4PWRC40yWwvvSn",
      "authorship_tag": "ABX9TyOu1mIP07GSrpF9JBmQOnmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}