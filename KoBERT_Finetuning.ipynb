{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1QzzLStU34g1LMHFs-H4PWRC40yWwvvSn",
      "authorship_tag": "ABX9TyOf3rSq/6rYx03SF6zXJJfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssanghn/AI-Emotion-Diary/blob/main/KoBERT_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBERT 감정 다중분류모델"
      ],
      "metadata": {
        "id": "XE0cy30W52Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> KoBERT를 이용하여 한국어 문장을 여러 클래스로 분류하는 모델을 만들어 보려고 한다.\n",
        "</br>\n",
        "공포, 놀람, 분노, 슬픔, 중립, 기쁨, 혐오와 같은 감정이 느껴지는 짧은 대화 텍스트를 각각 어떠한 감정의 텍스트인지 분류하는 모델을 만드는 것이다. </br>\n",
        "AIHUB에서 가져온 데이터셋으로 KoBERT 모델을 FineTuning 하여 </br>\n",
        "최종적으로 AI 감정일기 어플리케이션에서 작성한 일기 텍스트에서 감정을 추출하기 위해 프로젝트를 진행하였다."
      ],
      "metadata": {
        "id": "pyb4ZGUx5uRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoBERT란?**\n",
        "\n",
        "> BERT는 약 33억 개의 단어로 pretrain 되어 있는 기계번역 모델이다. </br>\n",
        "하지만 외국에서 만든 것이다 보니 영어에 대해서는 정확도는 높고, 한국어에 대해서는 정확도가 떨어진다.</br>\n",
        "따라서 BERT 모델을 한국어에도 잘 활용할 수 있도록 만들어진 것이 KoBERT이다. </br>\n",
        "KoBERT는 BERT 모델에서 한국어 데이터를 추가로 학습시킨 모델로, 한국어 위키에서 5백만개의 문장과 5천4백만개의 단어를 학습시킨 모델이다. </br>\n",
        "따라서 한국어 데이터에 대해서도 높은 정확도를 낼 수 있다고 한다. </br>\n",
        "한국어의 불규칙한 언어 변화의 특성을 반영하기 위해 데이터 기반 토큰화 (Tokenization) 기법을 적용하여 기존 대비 27%의 토큰만으로 2.6% 이상의 성능 향상을 이끌어 낸 모델이다. </br>"
      ],
      "metadata": {
        "id": "12P4-SnE6m5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 프로젝트 설명"
      ],
      "metadata": {
        "id": "wZ6mfcOv7_7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoBERT 파인튜닝**\n",
        "> BERT 모델은 자신의 사용 목적에 따라 파인튜닝(FineTuning)이 가능하다. </br>\n",
        "따라서 Output layer만 추가로 달아주면 원하는 결과를 출력해내는 기계번역 모델을 만들어 낼 수 있다. </br>\n",
        "이 점을 활용하여 KoBERT 모델을 FineTuning하여 최종적으로 일기 텍스트에서 감정을 추출해내는 모델을 새로 만들어 보려고 한다."
      ],
      "metadata": {
        "id": "kZHpaww98FmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 수집"
      ],
      "metadata": {
        "id": "ChrLMdP78T3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**데이터 수집**\n",
        "> 이번 프로젝트를 위해 사용한 한국어 대화 문장 데이터셋은 총 2개이다. </br>\n",
        "</br>\n",
        "1. 한국어 감정 정보가 포함된 단발성 대화 데이터셋\n",
        "* SNS 글 및 온라인 댓글에 대한 웹 크롤링을 실시하여 선정된 AIHUB의 공공 데이터셋\n",
        "* 약 3만 9천건의 데이터 셋 (기쁨, 슬픔, 놀람, 분노, 공포, 혐오, 중립의 7개 감정)\n",
        "</br>\n",
        "</br>\n",
        "2. 감성 대화 말뭉치\n",
        "* 한국어 감성 데이터셋을 상세한 감정 라벨링(감정 대분류와 소분류로 구분)과 함께 제공하는 AIHub의 공공 데이터셋\n",
        "* 약 4만 건의 데이터 셋 (기쁨, 슬픔, 당황, 분노, 불안, 상처의 6개 감정)\n",
        "* 데이터 병합을 위해 감정 대분류와 대화 문장을 남기고, 감정 소분류는 누락"
      ],
      "metadata": {
        "id": "TIL1jkIi8VkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab 환경 설정"
      ],
      "metadata": {
        "id": "7lWJDQw9MH_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델을 구현하기에 앞서 코랩 환경설정을 진행하려고 한다. </br>\n",
        "해당 사양은 KoBERT 오픈소스 내 requirements.txt를 참고하였다."
      ],
      "metadata": {
        "id": "Vk3XBoPUMyEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 환경 설정\n",
        "# requirements : https://github.com/SKTBrain/KoBERT/blob/master/kobert_hf/requirements.txt\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install pandas tqdm\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install transformers==4.8.2\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "k77lj1w6MNwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 다음으로 깃허브 내 파일을 Colab으로 가져온다."
      ],
      "metadata": {
        "id": "a1cNqF8fNL9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/SKTBrain/KoBERT의 파일들을 Colab으로 다운로드\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "255FRpYqNOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 그 다음으로는 해당 라이브러리들을 import 해준다."
      ],
      "metadata": {
        "id": "h26pFd-GPTsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "# Transformers\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "# Setting Library\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "W0nK9ZNXPWhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "vpfSUpg3ZQBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "\n",
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "QgZxLJSwZgat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTSentenceTransform:\n",
        "    \"\"\"BERT style data transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_seq_length, vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "        sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 2 strings:\n",
        "        text_a, text_b.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "\n",
        "        For single sequences, the input is a tuple of single string:\n",
        "        text_a.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
        "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
        "            string: (text_a,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "\n",
        "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
        "        # pre-training and are added to the wordpiece embedding vector\n",
        "        # (and position vector). This is not *strictly* necessary since\n",
        "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        #vocab = self._tokenizer.vocab\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')"
      ],
      "metadata": {
        "id": "xH7LJsWRNXMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "hbdF-_AzkQEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> BERT 데이터셋을 구성하는 라벨의 개수를 num_classes로 넣어주거나 모델을 통해 직접 추론할 수도 있다."
      ],
      "metadata": {
        "id": "EYYxbIrokhtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class BERTClassifier\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "crXgT2KcX1vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "KX70R4JFkWvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    dataset = [[sentence, '0']]\n",
        "    test = BERTDataset(dataset, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=2)\n",
        "    model.eval()\n",
        "    answer = 0\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        for logits in out:\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            answer = np.argmax(logits)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "LrgYhKJzkcmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 주어진 문장이 현재 학습이 완료된 모델 내에서 어떤 라벨과 argmax인지 판단하고 추론된 결과를 리턴하는 함수"
      ],
      "metadata": {
        "id": "NfmumGYWkzLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_excel('/content/drive/MyDrive/dataset/한국어_단발성_대화_데이터셋.xlsx')\n",
        "train = pd.read_excel('/content/drive/MyDrive/dataset/감성대화말뭉치Training.xlsx')\n",
        "test = pd.read_excel('/content/drive/MyDrive/dataset/감성대화말뭉치Validation.xlsx')\n",
        "\n",
        "data2 = pd.concat([train, test])\n",
        "\n",
        "\n",
        "# 감성 레이블을 정수로 변환 (0~6)\n",
        "data1.loc[(data1['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
        "data1.loc[(data1['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "data1.loc[(data1['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "data1.loc[(data1['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "data1.loc[(data1['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
        "data1.loc[(data1['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
        "data1.loc[(data1['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
        "\n",
        "data_list1 = []\n",
        "for q, label in zip(data1['Emotion'], data1['Sentence'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list1.append(data)\n",
        "\n",
        "# 감성 레이블을 정수로 변환 (0, 2, 3, 5)\n",
        "data2.loc[(data2['감정_대분류'] == \"불안\"), '감정_대분류'] = 0 #불안 => 0\n",
        "data2.loc[(data2['감정_대분류'] == \"분노\"), '감정_대분류'] = 2 #분노 => 2\n",
        "data2.loc[(data2['감정_대분류'] == \"슬픔\"), '감정_대분류'] = 3 #슬픔 => 3\n",
        "data2.loc[(data2['감정_대분류'] == \"상처\"), '감정_대분류'] = 3 #상처 => 3\n",
        "data2.loc[(data2['감정_대분류'] == \"기쁨\"), '감정_대분류'] = 5 #기쁨 => 5\n",
        "\n",
        "data_list2 = []\n",
        "for q, label in zip(data2['감정_대분류'], data2['사람문장1'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list2.append(data)\n",
        "\n",
        "merge_data = data_list1 + data_list2"
      ],
      "metadata": {
        "id": "AJwTrv_1nhJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = train_test_split(merge_data, test_size=0.2, shuffle=True, random_state=20)\n",
        "train_set, validation_set = train_test_split(train_set, test_size=0.25, shuffle=True, random_state=20)\n",
        "\n",
        "data_train = BERTDataset(train_set, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(test_set, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=2)"
      ],
      "metadata": {
        "id": "pJN9dSjgB9VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 한국어 단발성 대화 데이터셋 (7) </br>\n",
        ": 공포 놀람 분노 슬픔 중립 행복 혐오 </br>\n",
        "감성대화말뭉치 (6) </br>\n",
        ": 기쁨 당황 분노 불안 상처 슬픔 </br>\n",
        "</br>\n",
        "<병합된 결과> </br>\n",
        "한국어 단발성 대화 데이터셋 (7) </br>\n",
        ": 공포 놀람 분노 행복 슬픔   중립 혐오 (7) </br>\n",
        ": 불안 (----)  분노 기쁨 슬픔,상처 (5) </br>\n",
        "\n",
        "\n",
        "데이터셋을 살펴본 결과, 감성대화말뭉치의 '당황' 감정은 한국어 단발성 대화 데이터셋의 '공포', '놀람' 감정과 중복된다. </br>\n",
        "레이블들이 겹치므로 감성대화말뭉치의 '당황' 감정은 제거하고 두 데이터셋을 병합하였다.\n",
        "\n"
      ],
      "metadata": {
        "id": "vNJLehSh7QZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "bWYGE5HNnhlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, f'/content/drive/MyDrive/Colab Notebooks/SentimentAnalysisKOBert.pt')\n",
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/SentimentAnalysisKOBert_StateDict.pt')"
      ],
      "metadata": {
        "id": "aq-zP0Gznqqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}